{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capstone: Predicting Quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The goal of this project is to use both supervised and unsupervised machine learning models to explore a collection of quotes by famous people of various backgrounds and professions and address two problem statements.\n",
    "\n",
    "- **Problem Statement 1:  Given a collection of quotes, can the author of the quote be identified based on content and style.**\n",
    "\n",
    "- **Problem Statement 2:  Given a collection of quotes, can a set of common topics be identified.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "The source for the quote dataset will be the Successories web collection of most popular quotes:\n",
    "https://www.successories.com/iquote/authors/most\n",
    "\n",
    "Quotes will be scraped for all authors from A to Z.  Attributes of interest will be:\n",
    "\n",
    "- author\n",
    "- quote\n",
    "- ~~categories~~ _Decided no on categories since they are somewhat random and not helpful._\n",
    "\n",
    "#### Scraping Plan\n",
    "\n",
    "The authors quote site is somewhat deeply nested, but data will only need to be scraped once as the content is relatively static. I will find the link for each letter A-Z, and then links for each letter's subcategories. Then I will scrape the quotes for each author in the subcategory. The individual author's quote may be several pages deep.  Note several authors in the list only have a small number of quotes.  A lower bound will be set for Problem Statement 1 where authors will have to have at least #? quotes. \n",
    "\n",
    "\n",
    "#### Modeling\n",
    "\n",
    "The question around Problem Statement 1 is a supervised learning issue that involves multiple classification where each author represents a category.  Models used will most likely be MultinomialNB (Naive Bayes) classifier, RandomForestClassifier and LogisticRegression.\n",
    "\n",
    "The question around Problem Statement 2 is an unsupervised learning issue that involes topic modeling. An LDA (Latent Dirichlet Allocation) model will be used to explore this issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML for site is not well done.  The div includes several groups of links addressing different categories.  Classes were not used to help categorize the 'a' tags, so there is no easy way to distinguish/extract the href urls of interest. Extraction is based on link text being an uppercase alpha letter.\n",
    "\n",
    "``` html\n",
    "<a href=\"https://www.successories.com/iquote/authors/ka/kb\">K</a>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QuoteWebScraper(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self\n",
    "        \n",
    "    def get_author_quotes(self, letters):\n",
    "        count = 0\n",
    "        benchmark = 50\n",
    "        self.auth_urls_ = []\n",
    "        self.author_quotes_ = []\n",
    "        self.subindexes_ = []\n",
    "        alphabet_index = self.get_index_urls()\n",
    "        for letter in letters:\n",
    "            if letter.upper() in alphabet_index:\n",
    "                url = alphabet_index[letter.upper()]\n",
    "                self.subindexes_.extend(self.get_subindex_urls(url))\n",
    "                \n",
    "        [self.auth_urls_.extend(self.get_urls_for_authors(ilnk_)) for ilnk_ in self.subindexes_] \n",
    "        #[self.author_quotes_.extend(self.get_all_author_quotes(auth_url)) for auth_url in self.auth_urls_]\n",
    "        print('Begin quote extraction...')\n",
    "        for auth_url in self.auth_urls_:\n",
    "            count = count + 1\n",
    "            self.author_quotes_.extend(self.get_all_author_quotes(auth_url))\n",
    "            if (count % benchmark) == 0:\n",
    "                print(count, \"authors processed... \")\n",
    "                \n",
    "        print(\"Total authors processes=\", count)\n",
    "        return self.author_quotes_\n",
    "        \n",
    "    def get_alphabet(self, case='upper'):\n",
    "        self.alphabet = []\n",
    "        if case.lower() == 'upper':\n",
    "            range_ = range(65, 91)\n",
    "        elif case.lower() == 'lower':\n",
    "            range_ = range(97,123)\n",
    "\n",
    "        for letter in range_:\n",
    "            self.alphabet.append(chr(letter))\n",
    "        return self.alphabet\n",
    "    \n",
    "    def get_index_urls(self):\n",
    "        index_urls = {}\n",
    "        alphabet = self.get_alphabet() \n",
    "        url = 'https://www.successories.com/iquote/authors/most'\n",
    "        soup = self.create_beautifulSoup(url)\n",
    "        content_element = soup.find('div', {'class': 'quotedb_content'})\n",
    "        links = content_element.find_all('a')\n",
    "        for link in links:\n",
    "            if link.text in alphabet:\n",
    "                index_urls[link.text] = link['href']\n",
    "        return index_urls\n",
    "    \n",
    "    def get_subindex_urls(self, index_url):\n",
    "        subindex_urls = []\n",
    "        soup = self.create_beautifulSoup(index_url)\n",
    "        content_element = soup.find('div', {'class': 'quotedb_content'})\n",
    "        subindex = content_element.find('p').find_all('a')\n",
    "        # TODO list comprehension\n",
    "        for i in subindex:\n",
    "            link = i['href'].lower()\n",
    "            subindex_urls.append(link)\n",
    "        return subindex_urls\n",
    "    \n",
    "    def get_urls_for_authors(self, subindex_url):\n",
    "        self.author_urls = []\n",
    "        a_tags = []\n",
    "        soup = self.create_beautifulSoup(subindex_url)\n",
    "        results_div = soup.find('div', {'class', 'quotedb_navresults'})\n",
    "        author_divs = results_div.find_all('div', {'class', 'quotedb_navlist'})\n",
    "        [a_tags.extend(div.find_all('a')) for div in author_divs]\n",
    "        self.author_urls = [a['href'] for a in a_tags]\n",
    "        return self.author_urls\n",
    "    \n",
    "    def get_all_author_quotes(self, auth_url):\n",
    "        all_author_quotes = []\n",
    "        #print(auth_url)\n",
    "        while auth_url: # is not None: #True:\n",
    "            soup = self.create_beautifulSoup(auth_url)\n",
    "            all_author_quotes.extend(self.__get_page_quotes(soup))\n",
    "            pages = soup.find('ul', {'class', 'pager'})\n",
    "            if pages is not None:\n",
    "                next_ = pages.find('a', attrs={'class':'pager-link', 'rel':'next'})\n",
    "                if next_ is not None:\n",
    "                    auth_url = next_['href']\n",
    "                else:\n",
    "                    auth_url = None\n",
    "            else:\n",
    "                auth_url = None\n",
    "        return all_author_quotes\n",
    "    \n",
    "    def __get_page_quotes(self, soup):\n",
    "        self.quotes = []\n",
    "        author_name = soup.find('div', {'class', 'quotedb_quotelist'}).find('h1').find('a').text.replace(' Quotes', '')\n",
    "        quote_divs = soup.find_all('div', {'class', 'quote'})\n",
    "        for div in quote_divs:\n",
    "            self.quotes.append({'author':author_name.strip(), 'quote':div.find('a').text.replace('\"', '').strip()})\n",
    "        return self.quotes\n",
    "  \n",
    "    def create_beautifulSoup(self, url):\n",
    "        html = self.scrape_url(url)\n",
    "        return BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    def scrape_url(self, url, req_delay_sec=.500):\n",
    "        '''Method will GET the submitted url and return it's content. \n",
    "        The request is delayed by .5 seconds by default.  \n",
    "        To turn off any delay sumbit a value of 0. '''\n",
    "        if url:\n",
    "            if req_delay_sec > 0:\n",
    "                time.sleep(req_delay_sec)\n",
    "            resp = requests.get(url, headers = \n",
    "                                {'User-agent': 'pyeduquotereader:v0.0 (by /u/jmkds)',\n",
    "                                 'Cache-Control': 'no-cache'})\n",
    "            if resp.status_code == 200:\n",
    "                return resp.content\n",
    "            else:\n",
    "                print('Unable to get data due to failed request.  Status code: ', resp.status_code)\n",
    "                print('Error details: ', resp.text)\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "q_scraper = QuoteWebScraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run scraper for each letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin quote extraction...\n",
      "50 authors processed... \n",
      "Total authors processes= 68\n"
     ]
    }
   ],
   "source": [
    "# a, b, c, d, e, f, g, ...\n",
    "letters = 'iq'\n",
    "quotes_ = q_scraper.get_author_quotes(list(letters))\n",
    "\n",
    "# !! Write data to file\n",
    "write_date_to_file(quotes_, letters + '_quotes.csv')\n",
    "#pd.DataFrame(quotes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_date_to_file(data, filename, directory='../quote_data/'):\n",
    "    '''Write data to csv file.  Note, method will created directory if it does not exist.'''\n",
    "    df_data = pd.DataFrame(data)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(directory, ' created!')\n",
    "    df_data.to_csv(directory + filename, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../quote_data/*.csv\n",
      "../quote_data/a_quotes.csv\n",
      "../quote_data/b_quotes.csv\n",
      "../quote_data/c_quotes.csv\n",
      "../quote_data/d_quotes.csv\n",
      "../quote_data/e_quotes.csv\n",
      "../quote_data/f_quotes.csv\n",
      "../quote_data/g_quotes.csv\n",
      "../quote_data/h_quotes.csv\n",
      "../quote_data/iq_quotes.csv\n",
      "../quote_data/j_quotes.csv\n",
      "../quote_data/k_quotes.csv\n",
      "../quote_data/l_quotes.csv\n",
      "../quote_data/m_quotes.csv\n",
      "../quote_data/n_quotes.csv\n",
      "../quote_data/o_quotes.csv\n",
      "../quote_data/p_quotes.csv\n",
      "../quote_data/r_quotes.csv\n",
      "../quote_data/s_quotes.csv\n",
      "../quote_data/t_quotes.csv\n",
      "../quote_data/uv_quotes.csv\n",
      "../quote_data/w_quotes.csv\n",
      "../quote_data/xyz_quotes.csv\n"
     ]
    }
   ],
   "source": [
    "def create_dataset_from_csv(directory_):\n",
    "    path = r'../' + directory_\n",
    "    print(path + '/*.csv')\n",
    "    allFiles = glob.glob(path + '/*.csv')\n",
    "    list_ = []\n",
    "    for file_ in allFiles:\n",
    "        print(file_)\n",
    "        df = pd.read_csv(file_,index_col=None, header=0)\n",
    "        list_.append(df)\n",
    "        df_all = pd.concat(list_)\n",
    "    return df_all\n",
    "\n",
    "df_all = create_dataset_from_csv(\"quote_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>quote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alvar Aalto</td>\n",
       "      <td>Modern architecture does not mean the use of i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alvar Aalto</td>\n",
       "      <td>Building art is a synthesis of life in materia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alvar Aalto</td>\n",
       "      <td>We should concentrate our work not only to a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hank Aaron</td>\n",
       "      <td>I'm here to support the commissioner and tough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hank Aaron</td>\n",
       "      <td>That's going to be left up to the commissioner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hank Aaron</td>\n",
       "      <td>That's going to be left up to the commissioner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hank Aaron</td>\n",
       "      <td>I have always felt that although someone may d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hank Aaron</td>\n",
       "      <td>I think it's very much a distraction to the ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hank Aaron</td>\n",
       "      <td>Discover Greatness: An Illustrated History of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hank Aaron</td>\n",
       "      <td>It took me seventeen years to get 3,000 hits i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author                                              quote\n",
       "0  Alvar Aalto  Modern architecture does not mean the use of i...\n",
       "1  Alvar Aalto  Building art is a synthesis of life in materia...\n",
       "2  Alvar Aalto  We should concentrate our work not only to a s...\n",
       "3   Hank Aaron  I'm here to support the commissioner and tough...\n",
       "4   Hank Aaron  That's going to be left up to the commissioner...\n",
       "5   Hank Aaron  That's going to be left up to the commissioner...\n",
       "6   Hank Aaron  I have always felt that although someone may d...\n",
       "7   Hank Aaron  I think it's very much a distraction to the ba...\n",
       "8   Hank Aaron  Discover Greatness: An Illustrated History of ...\n",
       "9   Hank Aaron  It took me seventeen years to get 3,000 hits i..."
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
